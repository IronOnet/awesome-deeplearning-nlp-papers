### Flashes in NLP
- [ ] **`ERNIE`** ERNIE: Enhanced Language Representation with Informative Entities(2019)[[paper]](https://arxiv.org/abs/1905.07129)[[source]](https://github.com/thunlp/ERNIE)
- [ ] **`MASS`** MASS: Masked Sequence to Sequence Pre-training for Language Generation(2019) [[paper]](https://arxiv.org/abs/1905.02450)
- [x] **`BERT`** BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2019)[[paper]](https://arxiv.org/abs/1810.04805) [[source]](https://github.com/google-research/bert)
- [x] **`ATTENTION`** An Attentive Survey of Attention Models(2019) [[paper]](https://arxiv.org/abs/1904.02874)
- [x] **`ELMO`** Deep contextualized word representations(2018) [[paper]](https://arxiv.org/pdf/1802.05365.pdf)[[source]](https://github.com/allenai/bilm-tf)
- [x] **`ULMFiT`** Universal Language Model Fine-tuning for Text Classification(2018)[[paper]](https://arxiv.org/abs/1801.06146)
- [x] **`ATTENTION`** An Introductory Survey on Attention Mechanisms in NLP Problems(2018)[[paper]](https://arxiv.org/pdf/1811.05544.pdf)
- [x] **`Transformer`** [x] Attention is All you Need(2017) [[paper]](https://arxiv.org/abs/1706.03762)[[source]](https://github.com/tensorflow/tensor2tensor)
- [x] **`Transformer-XL`** Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context
- [x] **`GPT`** Improving Language Understanding by Generative Pre-Training(2018)[[paper]](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [x] **`GPT-2`** Language Models are Unsupervised Multitask Learners(2019) [[paper]](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)[[source]](https://github.com/openai/gpt-2)
- [x] **`ConvSeq2Seq`** Convolutional Sequence to Sequence Learning(2017) [[paper]](https://arxiv.org/abs/1705.03122)
- [ ] Unsupervised Pretraining for Sequence to Sequence Learning(2017)[[paper]](https://www.aclweb.org/anthology/D17-1039)
- [ ] **`Graph2Seq`** Graph2Seq: Graph to Sequence Learning with Attention-based Neural Networks(2018)[[paper]](https://arxiv.org/abs/1804.00823)
- [ ] **`Memory Networks`** End-To-End Memory Networks(2015) [[paper]](https://arxiv.org/pdf/1503.08895.pdf)
- [ ] **`SeqGAN`** SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient(2017) [[paper]](https://arxiv.org/pdf/1609.05473.pdf)
- [ ] An Overview of Multi-Task Learning in Deep Neural Networks(2017)[[paper]](https://arxiv.org/pdf/1706.05098.pdf)
- [ ] Latent Multi-task Architecture Learning(2019)[[paper]](https://arxiv.org/pdf/1705.08142.pdf)
- [ ] A Unified Perspective on Multi-Domain and Multi-Task Learning(2015)[[paper]](https://arxiv.org/pdf/1412.7489.pdf)
